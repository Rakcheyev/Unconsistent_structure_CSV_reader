Metadata-Version: 2.4
Name: unconsistent-structure-csv-reader
Version: 0.1.0
Summary: Resource-aware tool for analyzing and normalizing CSV files with inconsistent structure.
Author: Data Engineering Tools
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Provides-Extra: dev
Requires-Dist: ruff<0.7,>=0.6.0; extra == "dev"
Requires-Dist: mypy<1.11,>=1.10; extra == "dev"
Requires-Dist: pytest<9.0,>=8.3; extra == "dev"

# Unconsistent Structure CSV Reader (Python Skeleton)

Цей репозиторій містить каркас інструменту для двофазного аналізу й нормалізації CSV/TSV файлів із непослідовною структурою. Поточна версія фокусується на Python-реалізації з чітким поділом на UI, Core Engine та Storage/Config рівні.

## Архітектурні блоки
- **UI** – WPF/WinUI-подібний досвід, зараз описаний як Python-заглушка з вимогами до візуальних сцен, прогрес-барів та workflow-driven UX.
- **Core Engine** – модулі `analysis`, `mapping`, `normalization`, `materialization`, які реалізують відповідні фази (Phase 1, Phase 1.5, Phase 2) і працюють паралельно по файлах/схемах.
- **Storage & Config** – JSON/SQLite-шар для схем, мапінгу блоків, статистики колонок і словників синонімів.
- **Docs** – детальні вимоги до UX, алгоритмів семплінгу та дизайн-рішення загалом.

## Поточний статус
- Створені pythonic `dataclasses` у `src/common/models.py`, які відповідають описаним C# моделям.
- Для кожного блоку додано `TASKS.md` з конкретними TODO і гранулярністю, достатньою для планування спринтів.
- Файл `AGENTS.md` містить правила координації між агендами/модулями.
- Додано `pyproject.toml` з мінімальними залежностями та конфігами `ruff`/`mypy` для суворого контролю якості на Python 3.11.
- Створено `config/defaults.json` із профілями для систем із низькими ресурсами та робочих станцій.
- Підготовлено bootstrap-скрипти (`scripts/bootstrap_env.ps1` / `.sh`) і smoke-тести в `tests/` із крихітними CSV-фікстурами.
- Фаза 1 проаналізована з адаптивним throttling, структурованими JSONL прогрес-логами та CLI-бенчмаркером.
- Phase 2 отримала перший реальний job runner: chunked CSV writers (відновлення з checkpoint, максимум дві паралельні схеми) і SQLite-аудит опційно з кожної CLI-команди.

## Наступні кроки
1. Додати lightweight колонний профайлер (reservoir sampling + HLL-lite) і метрики рядків/байтів у SQLite.
2. Підготувати Parquet/bulk-DB writer за тим самим інтерфейсом, включно з throttlingом.
3. Розширити `docs/ui_ux.md` моками для schema cards + merge dialog (див. також `docs/schema_review_brief.md`).
4. Автоматизувати прогін recovery tests: симулювати падіння в середині блока та відновлення з checkpoint.

## Hardware Profiles & Setup

| Profile        | Target hardware                 | block_size | max_parallel_files | sample_values_cap |
| -------------- | ------------------------------- | ---------- | ------------------ | ----------------- |
| `low_memory`   | 2-core CPU, ≤2 GB RAM, HDD      | 1 000      | 1                  | 24                |
| `workstation`  | 8-core CPU, ≥16 GB RAM, SSD     | 10 000     | 4                  | 64                |

Параметри лежать у `config/defaults.json` і можуть бути підлаштовані під конкретні кластери/ноутбуки. Під час ранньої експлуатації рекомендується починати з `low_memory`, щоб уникнути пікових викидів RAM/IO, а потім підвищувати паралельність.

## Fast Bootstrap

```powershell
PS> scripts/bootstrap_env.ps1 -Dev
```

```bash
$ ./scripts/bootstrap_env.sh --dev
```

Скрипти створюють `.venv`, оновлюють `pip` і ставлять поточний пакет (з dev-інструментами за потреби). Це гарантує однакове оточення навіть на версіях Windows Server або мінімальних Linux-боксах.

## CLI Workflow Shell

Після bootstrap доступний покроковий CLI (`uscsv`) для всього сценарію Import → Analyze → Review → Normalize → Materialize:

1. **Аналіз** — `uscsv analyze data/raw --profile low_memory --output mapping.json --progress-log artifacts/progress.jsonl --sqlite-db artifacts/storage.db`. Команда читає файли паралельно, стрімить прогрес у консоль + JSONL та одразу синхронізує SQLite, якщо передано `--sqlite-db`.
2. **Бенчмарк** — `uscsv benchmark data/raw --profile workstation --log artifacts/bench.jsonl` для порівняння throughput профілів/машин.
3. **Рев'ю / кластеризація** — `uscsv review mapping.json --output mapping.review.json --synonyms storage/synonyms.json --sqlite-db artifacts/storage.db` запускає heuristic clustering і синхронізує результати в SQLite.
4. **Нормалізація назв** — `uscsv normalize mapping.review.json --output mapping.normalized.json --sqlite-db artifacts/storage.db` застосовує словник синонімів.
5. **Матеріалізація (реальний writer)** — `uscsv materialize mapping.normalized.json --dest artifacts/output --checkpoint artifacts/materialize_checkpoint.json --plan artifacts/materialization_plan.json --sqlite-db artifacts/storage.db`. Команда створює chunked CSV (за замовчуванням по `writer_chunk_rows`), не тримає більше двох активних writers та зберігає checkpoint після кожного блока. Повторний запуск продовжить із останнього незавершеного блока.

Прапорець `--include-samples` на кожному етапі додає в JSON обмежену кількість `sample_values` (корисно для дебагу, але збільшує розмір файлу).

## SQLite persistence та словник синонімів

- Файл `storage/synonyms.json` містить просту мапу `NormalizedName -> [variants...]`. CLI автоматично підтягує його для команд `review` та `normalize`, але за потреби шлях можна перевизначити через `--synonyms`.
- Параметр `--sqlite-db` (опційний для analyze/review/normalize/materialize) створює/оновлює SQLite із таблицями `schemas`, `blocks`, `audit_log`. Кожна фаза логуватиме подію (entity `mapping` або `materialization`) з timestamp, тому UI може відображати історію job'ів без зчитування великих JSON.

## Smoke Tests

Папка `tests/` містить перші smoke-тести й фікстури:

- `tests/data/retail_small.csv` — крихітний датасет для перевірки пайплайнів без великого IO.
- `tests/test_config_profiles.py` — гарантує, що всі профілі мають необхідні поля й можуть бути прочитані в CI (<1 s).

Запуск: `python -m pytest tests -q` (після bootstrap).

## Tracking Progress

- Операційні плани зберігаються в `TODO.md`.
- Детальний журнал змін — у `CHANGELOG.md`.
